import streamlit as st
import time
import json
import requests
import hashlib
from requests.auth import HTTPBasicAuth
from couchbase.cluster import Cluster
from couchbase.options import ClusterOptions
from couchbase.auth import PasswordAuthenticator
import boto3
import json as json_lib

# === CONFIG ===
CB_CONN_STRING = "couchbase://localhost"
CB_BUCKET = "block_convo"
CB_USERNAME = "chatbot"
CB_PASSWORD = "chatbot123"
CB_FTS_URL = "http://localhost:8094/api/index/chat_vector_index/query"

BEDROCK_REGION = "us-east-1"
EMBED_MODEL_ID = "amazon.titan-embed-text-v1"
LLM_MODEL_ID = "meta.llama3-70b-instruct-v1:0"

# === BEDROCK CLIENT ===
bedrock_runtime = boto3.client("bedrock-runtime", region_name=BEDROCK_REGION)

# === UTILS ===
def hash_question(question):
    return hashlib.sha256(question.strip().lower().encode("utf-8")).hexdigest()

def get_embedding(text):
    body = {"inputText": text}
    try:
        response = bedrock_runtime.invoke_model(
            modelId=EMBED_MODEL_ID,
            body=json_lib.dumps(body),
            contentType="application/json",
            accept="application/json"
        )
        response_body = json_lib.loads(response["body"].read())
        return response_body.get("embedding")
    except Exception as e:
        st.error(f"Error getting embedding: {e}")
        return None

def generate_answer(question):
    prompt_text = f"Answer concisely:\n\n{question}\n\nAnswer:"
    body = {"prompt": prompt_text}
    try:
        response = bedrock_runtime.invoke_model(
            modelId=LLM_MODEL_ID,
            body=json_lib.dumps(body),
            contentType="application/json",
            accept="application/json"
        )
        response_body = json_lib.loads(response["body"].read())
        generated = response_body.get("generation") or (response_body.get("generations") or [{}])[0].get("text")
        return generated.strip() if generated else "No answer generated."
    except Exception as e:
        st.error(f"Error generating answer: {e}")
        return None

def vector_search_rest(embedding_vector, k=3):
    query_json = {
        "fields": ["*"],
        "knn": [
            {
                "k": k,
                "field": "embedding_vector",
                "vector": embedding_vector
            }
        ]
    }
    headers = {"Content-Type": "application/json"}
    try:
        response = requests.post(
            CB_FTS_URL,
            auth=HTTPBasicAuth(CB_USERNAME, CB_PASSWORD),
            headers=headers,
            data=json.dumps(query_json)
        )
        if response.status_code == 200:
            return response.json()
        else:
            st.error(f"Couchbase vector search error {response.status_code}: {response.text}")
            return None
    except Exception as e:
        st.error(f"Error calling vector search: {e}")
        return None

def connect_couchbase():
    cluster = Cluster(
        CB_CONN_STRING,
        ClusterOptions(PasswordAuthenticator(CB_USERNAME, CB_PASSWORD))
    )
    bucket = cluster.bucket(CB_BUCKET)
    return bucket.default_collection()

def store_qa(question, answer, embedding_vector):
    collection = connect_couchbase()
    hash_key = hash_question(question)
    doc_id = f"qa::{hash_key}"
    doc = {
        "type": "block_convo",
        "question": question,
        "answer": answer,
        "embedding_vector": embedding_vector
    }
    try:
        collection.upsert(doc_id, doc)
        st.write(f"‚úÖ Stored new doc under ID: {doc_id}")
    except Exception as e:
        st.error(f"Error inserting document: {e}")

def fetch_doc_by_key(question):
    try:
        collection = connect_couchbase()
        doc_id = f"qa::{hash_question(question)}"
        return collection.get(doc_id).content_as[dict]
    except Exception:
        return None

# === STREAMLIT UI ===
st.title("Semantic Search Chatbot üìöüîç")

user_question = st.text_input("Enter your question:")

if user_question:
    start_time = time.time()
    embedding = get_embedding(user_question)
    if not embedding:
        st.error("Failed to get embedding.")
        st.stop()
    st.write("üß¨ Embedding generated.")

    cb_results = vector_search_rest(embedding, k=3)
    vector_time = time.time() - start_time
    hits_count = len(cb_results['hits']) if cb_results and 'hits' in cb_results else 0
    st.write(f"üìä Vector search returned {hits_count} hits")

    top_hit_doc = None
    answer, source = None, None

    # Show top 3 question + score results
    if cb_results and "hits" in cb_results and cb_results["hits"]:
        for hit in cb_results["hits"]:
            score = hit.get("score", 0)
            try:
                collection = connect_couchbase()
                doc = collection.get(hit['id']).content_as[dict]
                # Show question + score only
                st.write(f"‚Ä¢ Question: {doc.get('question')} | Score: {score:.3f}")
                # Pick top doc if score >= 0.9 for answer
                if score >= 0.9 and not top_hit_doc:
                    top_hit_doc = doc
                    answer = doc.get("answer")
                    source = "Couchbase (vector search)"
            except Exception as e:
                st.error(f"Error fetching doc {hit['id']}: {e}")

    # Always do key cache lookup and time it
    keycache_start = time.time()
    cached_doc = fetch_doc_by_key(user_question)
    keycache_time = time.time() - keycache_start

    if not top_hit_doc and cached_doc:
        st.write("üîë Found doc in key cache.")
        top_hit_doc = cached_doc
        answer = cached_doc.get("answer")
        source = "Couchbase (key cache)"

    if not top_hit_doc:
        st.write("‚ùå No matching doc in vector or key cache.")
        answer = generate_answer(user_question)
        if answer:
            st.write("‚öôÔ∏è Storing new doc‚Ä¶")
            store_qa(user_question, answer, embedding)
            source = "Bedrock LLM"
        else:
            st.error("‚ùå Failed to generate answer. No doc stored.")
            st.stop()

    total_time = time.time() - start_time

    st.markdown(f"**Answer:** {answer}")
    st.markdown(f"*Source:* {source}")
    st.markdown(f"*Vector search time:* {vector_time:.3f} sec")
    st.markdown(f"*Key cache lookup time:* {keycache_time:.3f} sec")
    st.markdown(f"*Total response time:* {total_time:.2f} sec")
